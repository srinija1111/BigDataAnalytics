{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c854c4fe-b532-40fc-9a0f-13777852b616",
   "metadata": {},
   "source": [
    "**Perform simple data transformation like filtering even numbers from a given list using pyspark RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520a8027-4ba6-4798-9847-b917664a4a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-83C5GA1:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7f6a16-8d3f-4bb3-b05f-5b6a26b5165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b5820a-a00e-447e-bbbd-c47cbe17ef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original List:\n",
      "[465, 202, 241, 16, 143, 708, 826, 567, 242, 69, 10, 730, 543, 816, 553, 517, 882, 516, 366, 679, 201, 113, 119, 449, 731, 105, 231, 656, 406, 616, 291, 26, 706, 874, 58, 476, 448, 964, 7, 323, 239, 754, 440, 169, 226, 678, 816, 693, 71, 187, 645, 110, 443, 718, 251, 300, 248, 637, 821, 851, 173, 370, 787, 845, 407, 975, 553, 534, 861, 879, 838, 468, 904, 732, 377, 693, 532, 117, 419, 421, 865, 65, 702, 818, 622, 985, 959, 360, 441, 881, 132, 589, 762, 218, 409, 84, 879, 39, 693, 8]\n"
     ]
    }
   ],
   "source": [
    " random_numbers = [random.randint(1, 1000) for _ in range(100)]\n",
    " print(\"Original List:\")\n",
    " print(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a93661-4963-46fc-8bdb-17cd41f90b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rdd = sc.parallelize(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2523091a-c1fe-4ac5-b673-8d8e20198c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "even_numbers_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4fc567-b9a4-42cd-affa-74298a4f5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Even Numbers:\n",
      "[202, 16, 708, 826, 242, 10, 730, 816, 882, 516, 366, 656, 406, 616, 26, 706, 874, 58, 476, 448, 964, 754, 440, 226, 678, 816, 110, 718, 300, 248, 370, 534, 838, 468, 904, 732, 532, 702, 818, 622, 360, 132, 762, 218, 84, 8]\n"
     ]
    }
   ],
   "source": [
    " even_numbers = even_numbers_rdd.collect()\n",
    " print(\"\\nEven Numbers:\")\n",
    " print(even_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5590c44-1ce9-41ad-a5aa-1d205a9d6f31",
   "metadata": {},
   "source": [
    "**CONCLUSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f52f6a-e6d8-48f8-a1e4-d2c4f9cf79bd",
   "metadata": {},
   "source": [
    "This program demonstrates how PySparkâ€™s RDD operations can be used for simple data transformations. We started by generating a list of 100 random \n",
    "integers and then parallelized it into an RDD using the SparkContext. With the help of the filter() transformation, only the even numbers were selected,\n",
    "and finally, the collect() action brought the results back to the driver for display. This shows how PySpark enables us to process collections in a \n",
    "distributed and parallelized manner, making it efficient and scalable even for much larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67309a80-4fde-41fc-ac19-68c86092b768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
